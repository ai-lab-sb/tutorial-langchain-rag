"""
Tutorial Completo Paso a Paso de RAG con LangChain
Este script te guÃ­a a travÃ©s de cada componente del sistema RAG
con explicaciones detalladas y ejemplos interactivos.
"""
import time
from typing import List
from langchain.schema import Document
from data_loader import load_data_from_google_sheets
from rag_system import RAGSystem
import config


def print_header(title: str):
    """Imprime un encabezado formateado"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70 + "\n")


def print_step(number: int, title: str):
    """Imprime el nÃºmero de paso"""
    print(f"\n{'â”€'*70}")
    print(f"PASO {number}: {title}")
    print(f"{'â”€'*70}\n")


def pause(message: str = "Presiona Enter para continuar..."):
    """Pausa la ejecuciÃ³n"""
    input(f"\nâ¸ï¸  {message}")


def explain_rag():
    """Explica quÃ© es RAG"""
    print_header("ğŸ“ Â¿QUÃ‰ ES RAG?")
    
    print("""
RAG (Retrieval-Augmented Generation) es una tÃ©cnica que mejora las 
respuestas de los modelos de lenguaje grandes (LLMs) combinando dos pasos:

1. ğŸ“š RECUPERACIÃ“N (Retrieval):
   - Busca informaciÃ³n relevante en una base de conocimientos
   - Utiliza bÃºsqueda semÃ¡ntica (por significado, no solo palabras clave)
   - Recupera los documentos mÃ¡s similares a la pregunta

2. âœï¸  GENERACIÃ“N (Generation):
   - El LLM recibe la pregunta + los documentos recuperados
   - Genera una respuesta basada en la informaciÃ³n proporcionada
   - Produce respuestas mÃ¡s precisas y actualizadas

ğŸ¯ VENTAJAS DE RAG:
   âœ“ Respuestas basadas en datos especÃ­ficos (no alucinaciones)
   âœ“ InformaciÃ³n actualizada sin reentrenar el modelo
   âœ“ Fuentes verificables (sabes de dÃ³nde viene la informaciÃ³n)
   âœ“ Flexible (puedes cambiar la base de conocimientos fÃ¡cilmente)

ğŸ“Š FLUJO DE RAG:
   Pregunta â†’ Embedding â†’ BÃºsqueda en Vector Store â†’ 
   Recuperar Top-K docs â†’ Contexto + Pregunta â†’ LLM â†’ Respuesta
    """)


def explain_embeddings():
    """Explica quÃ© son los embeddings"""
    print_header("ğŸ§® EMBEDDINGS: Convirtiendo Texto en NÃºmeros")
    
    print("""
Los EMBEDDINGS son representaciones numÃ©ricas (vectores) de texto que 
capturan el significado semÃ¡ntico.

ğŸ”¢ CARACTERÃSTICAS:
   - Vector de nÃºmeros (ej: [0.2, -0.5, 0.8, ...])
   - DimensiÃ³n fija (ej: 768 o 1536 dimensiones)
   - Textos similares â†’ vectores cercanos
   - Captura contexto y relaciones

ğŸ“ EJEMPLO CONCEPTUAL:
   "perro"     â†’ [0.8, 0.2, 0.1, ...]
   "gato"      â†’ [0.7, 0.3, 0.15, ...] â† Cercano a "perro"
   "auto"      â†’ [0.1, 0.9, 0.8, ...] â† Lejano de "perro"

ğŸ¯ EN NUESTRO SISTEMA:
   Usamos el modelo de embeddings de Google (embedding-001)
   que convierte preguntas y documentos en vectores de 768 dimensiones.
    """)


def explain_faiss():
    """Explica quÃ© es FAISS"""
    print_header("âš¡ FAISS: BÃºsqueda UltrarrÃ¡pida de Vectores")
    
    print("""
FAISS (Facebook AI Similarity Search) es una biblioteca optimizada
para buscar vectores similares de manera eficiente.

ğŸš€ CARACTERÃSTICAS:
   - BÃºsquedas extremadamente rÃ¡pidas (millones de vectores)
   - Algoritmos optimizados (GPU y CPU)
   - Bajo uso de memoria
   - Varios tipos de Ã­ndices

ğŸ” CÃ“MO FUNCIONA:
   1. Indexa todos los embeddings de documentos
   2. Organiza los vectores para bÃºsqueda eficiente
   3. Cuando llega una consulta:
      - Calcula la distancia a todos los vectores
      - Retorna los K vectores mÃ¡s cercanos
      - Todo en milisegundos âš¡

ğŸ“Š MÃ‰TRICA DE SIMILITUD:
   Usamos "similitud coseno" que mide el Ã¡ngulo entre vectores:
   - Ãngulo pequeÃ±o = Alta similitud (vectores apuntan en la misma direcciÃ³n)
   - Ãngulo grande = Baja similitud

ğŸ’¾ EN NUESTRO SISTEMA:
   FAISS indexa los embeddings de Google Sheets y permite
   encontrar rÃ¡pidamente los documentos mÃ¡s relevantes.
    """)


def explain_langchain():
    """Explica quÃ© es LangChain"""
    print_header("ğŸ”— LANGCHAIN: Orquestando el Sistema RAG")
    
    print("""
LangChain es un framework para construir aplicaciones con LLMs.
Proporciona componentes modulares que se "encadenan" (chain).

ğŸ§© COMPONENTES PRINCIPALES:
   
   1. Documents: Unidades de informaciÃ³n con contenido + metadata
   
   2. Embeddings: Interfaz para modelos de embeddings
   
   3. Vector Stores: Almacenamiento de embeddings (FAISS, Pinecone, etc.)
   
   4. Retrievers: Recuperan documentos relevantes
   
   5. LLMs: Modelos de lenguaje (Gemini, GPT, etc.)
   
   6. Chains: Encadenan componentes para flujos complejos
   
   7. Prompts: Templates para estructurar las consultas

ğŸ”„ EN NUESTRO SISTEMA:
   LangChain conecta:
   Google Sheets â†’ Documents â†’ Embeddings â†’ FAISS â†’ 
   Retriever â†’ Gemini â†’ Respuesta
    """)


def demonstrate_loading_data():
    """Demuestra la carga de datos"""
    print_step(1, "Cargando Datos desde Google Sheets")
    
    print("""
Vamos a cargar los datos de tu Google Sheet.
El proceso es:
   1. Autenticar con las credenciales de la cuenta de servicio
   2. Conectar con la hoja usando el ID
   3. Leer todas las filas
   4. Convertir cada fila en un Document de LangChain

Cada Document tiene:
   - page_content: El texto del documento
   - metadata: InformaciÃ³n adicional (tema, pregunta, etc.)
    """)
    
    pause("Â¿Listo para cargar los datos?")
    
    print("\nğŸ”„ Cargando datos...\n")
    documents = load_data_from_google_sheets()
    
    if documents:
        print(f"\nâœ… Â¡Ã‰xito! Se cargaron {len(documents)} documentos")
        
        # Mostrar un ejemplo
        print("\nğŸ“„ Ejemplo de documento:")
        print("-" * 70)
        doc = documents[0]
        print(f"Contenido:\n{doc.page_content}\n")
        print(f"Metadata: {doc.metadata}")
        print("-" * 70)
        
        return documents
    else:
        print("\nâŒ No se pudieron cargar los datos")
        return None


def demonstrate_embeddings(documents: List[Document]):
    """Demuestra la creaciÃ³n de embeddings"""
    print_step(2, "Creando Embeddings")
    
    print("""
Ahora vamos a convertir el texto en vectores numÃ©ricos.

Proceso:
   1. Tomar cada documento
   2. Enviar el texto al modelo de embeddings de Google
   3. Recibir un vector de 768 nÃºmeros
   4. Este vector captura el "significado" del texto

Ejemplo (simplificado):
   Texto: "Python es un lenguaje de programaciÃ³n"
   Embedding: [0.23, -0.45, 0.67, 0.12, ..., 0.89] (768 nÃºmeros)
    """)
    
    pause("Â¿Listo para crear embeddings?")
    
    print("\nğŸ”„ Creando embeddings...")
    
    rag = RAGSystem()
    if rag.setup_embeddings():
        print("\nâœ… Modelo de embeddings configurado")
        print(f"   Modelo: {config.EMBEDDING_MODEL}")
        print("   DimensiÃ³n: 768")
        return rag
    else:
        print("\nâŒ Error al configurar embeddings")
        return None


def demonstrate_vectorstore(rag: RAGSystem, documents: List[Document]):
    """Demuestra la creaciÃ³n del vector store"""
    print_step(3, "Creando Vector Store con FAISS")
    
    print(f"""
Ahora indexaremos los {len(documents)} documentos en FAISS.

Proceso:
   1. Para cada documento:
      - Calcular su embedding
      - Agregar al Ã­ndice FAISS
   2. FAISS organiza los vectores para bÃºsqueda eficiente
   3. Guardar el Ã­ndice localmente para uso futuro

Esto puede tomar unos segundos...
    """)
    
    pause("Â¿Listo para crear el vector store?")
    
    print("\nğŸ”„ Creando Ã­ndice FAISS...")
    
    if rag.create_vectorstore(documents, save_local=True):
        print("\nâœ… Vector store creado y guardado")
        print(f"   Documentos indexados: {len(documents)}")
        print(f"   Guardado en: {config.FAISS_INDEX_PATH}")
        return True
    else:
        print("\nâŒ Error al crear vector store")
        return False


def demonstrate_llm(rag: RAGSystem):
    """Demuestra la configuraciÃ³n del LLM"""
    print_step(4, "Configurando el Modelo de Lenguaje (Gemini)")
    
    print(f"""
Configuraremos Gemini como nuestro modelo de lenguaje.

ConfiguraciÃ³n:
   - Modelo: {config.MODEL_NAME}
   - Temperature: {config.TEMPERATURE}
     (0 = mÃ¡s determinista, 1 = mÃ¡s creativo)
   - Provider: Google Generative AI

El LLM serÃ¡ el encargado de:
   1. Recibir la pregunta + contexto recuperado
   2. Comprender y razonar sobre la informaciÃ³n
   3. Generar una respuesta coherente y Ãºtil
    """)
    
    pause("Â¿Listo para configurar Gemini?")
    
    print("\nğŸ”„ Configurando Gemini...")
    
    if rag.setup_llm():
        print("\nâœ… Gemini configurado correctamente")
        return True
    else:
        print("\nâŒ Error al configurar Gemini")
        return False


def demonstrate_chain(rag: RAGSystem):
    """Demuestra la creaciÃ³n de la chain"""
    print_step(5, "Creando la Cadena RAG")
    
    print(f"""
Finalmente, conectaremos todos los componentes en una "chain".

La chain incluye:
   1. Retriever: Busca los top-{config.TOP_K_DOCUMENTS} documentos relevantes
   2. Prompt Template: Estructura la pregunta con el contexto
   3. LLM (Gemini): Genera la respuesta
   4. Output Parser: Formatea la salida

Flujo completo:
   Pregunta 
      â†“
   Embedding de la pregunta
      â†“
   BÃºsqueda en FAISS (retriever)
      â†“
   Recuperar documentos similares
      â†“
   Construir prompt: contexto + pregunta
      â†“
   Enviar a Gemini
      â†“
   Respuesta final
    """)
    
    pause("Â¿Listo para crear la chain?")
    
    print("\nğŸ”„ Creando chain RAG...")
    
    if rag.setup_qa_chain():
        print("\nâœ… Chain RAG lista para usar")
        print(f"   Documentos a recuperar: {config.TOP_K_DOCUMENTS}")
        return True
    else:
        print("\nâŒ Error al crear chain")
        return False


def demonstrate_queries(rag: RAGSystem):
    """Demuestra consultas al sistema"""
    print_step(6, "Â¡Probemos el Sistema RAG!")
    
    print("""
Ahora el sistema estÃ¡ completamente funcional.
Vamos a hacer algunas preguntas y ver cÃ³mo funciona.

Para cada pregunta verÃ¡s:
   1. La pregunta original
   2. La respuesta generada por Gemini
   3. Los documentos fuente que se usaron
    """)
    
    pause("Â¿Listo para las preguntas de prueba?")
    
    # Preguntas de ejemplo
    questions = [
        "Â¿QuÃ© es Python?",
        "ExplÃ­came quÃ© es RAG y por quÃ© es Ãºtil",
        "Â¿QuÃ© es FAISS y para quÃ© sirve?"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\n{'='*70}")
        print(f"PREGUNTA {i}/{len(questions)}")
        print(f"{'='*70}")
        
        print("\nğŸ” PROCESO INTERNO:")
        print("   1. Convertir pregunta a embedding...")
        print("   2. Buscar en FAISS los documentos mÃ¡s similares...")
        print("   3. Recuperar contexto relevante...")
        print("   4. Enviar a Gemini con el contexto...")
        print("   5. Generar respuesta...\n")
        
        time.sleep(1)  # Pausa dramÃ¡tica ğŸ˜„
        
        result = rag.query(question, verbose=True)
        
        if i < len(questions):
            pause("Presiona Enter para la siguiente pregunta...")


def main():
    """FunciÃ³n principal del tutorial"""
    print_header("ğŸ“ TUTORIAL COMPLETO DE RAG CON LANGCHAIN, FAISS Y GEMINI")
    
    print("""
Â¡Bienvenido al tutorial interactivo!

Este tutorial te llevarÃ¡ paso a paso a travÃ©s de:
   âœ“ Conceptos teÃ³ricos de RAG
   âœ“ Carga de datos desde Google Sheets
   âœ“ CreaciÃ³n de embeddings
   âœ“ Uso de FAISS como vector store
   âœ“ ConfiguraciÃ³n de Gemini
   âœ“ Consultas al sistema completo

DuraciÃ³n estimada: 10-15 minutos

âš ï¸  IMPORTANTE: AsegÃºrate de haber:
   1. Configurado tu API key de Gemini en .env
   2. Creado y configurado tu Google Sheet
   3. Descargado las credenciales de la cuenta de servicio
   4. Instalado todas las dependencias (pip install -r requirements.txt)
    """)
    
    pause("Â¿Listo para comenzar?")
    
    # SecciÃ³n 1: TeorÃ­a
    explain_rag()
    pause()
    
    explain_embeddings()
    pause()
    
    explain_faiss()
    pause()
    
    explain_langchain()
    pause()
    
    # SecciÃ³n 2: PrÃ¡ctica
    print_header("ğŸ’» PARTE PRÃCTICA: Construyendo el Sistema")
    
    # Paso 1: Cargar datos
    documents = demonstrate_loading_data()
    if not documents:
        print("\nâŒ No se puede continuar sin datos")
        return
    pause()
    
    # Paso 2: Embeddings
    rag = demonstrate_embeddings(documents)
    if not rag:
        print("\nâŒ No se puede continuar sin embeddings")
        return
    pause()
    
    # Paso 3: Vector Store
    if not demonstrate_vectorstore(rag, documents):
        print("\nâŒ No se puede continuar sin vector store")
        return
    pause()
    
    # Paso 4: LLM
    if not demonstrate_llm(rag):
        print("\nâŒ No se puede continuar sin LLM")
        return
    pause()
    
    # Paso 5: Chain
    if not demonstrate_chain(rag):
        print("\nâŒ No se puede continuar sin chain")
        return
    pause()
    
    # Paso 6: Queries
    demonstrate_queries(rag)
    
    # ConclusiÃ³n
    print_header("ğŸ‰ Â¡FELICITACIONES!")
    
    print("""
Â¡Has completado el tutorial de RAG con LangChain!

ğŸ¯ LO QUE APRENDISTE:
   âœ“ Conceptos fundamentales de RAG
   âœ“ CÃ³mo funcionan los embeddings
   âœ“ Uso de FAISS para bÃºsqueda vectorial
   âœ“ IntegraciÃ³n de componentes con LangChain
   âœ“ Uso de Gemini para generaciÃ³n de respuestas

ğŸš€ PRÃ“XIMOS PASOS:
   1. Agrega mÃ¡s datos a tu Google Sheet
   2. Experimenta con diferentes preguntas
   3. Ajusta los parÃ¡metros (temperature, top_k, etc.)
   4. Prueba el modo interactivo: python main.py --interactive
   5. Explora el cÃ³digo fuente para entender los detalles

ğŸ“š RECURSOS:
   - README.md: DocumentaciÃ³n completa
   - main.py: Modo interactivo
   - rag_system.py: ImplementaciÃ³n del sistema
   - data_loader.py: Carga de datos

ğŸ’¡ CONSEJOS:
   - Guarda el Ã­ndice FAISS para evitar recrearlo cada vez
   - Usa --use-existing-index para cargas mÃ¡s rÃ¡pidas
   - Experimenta con diferentes modelos de Gemini
   - Monitorea el uso de tu API key

Â¡Gracias por completar el tutorial!
    """)
    
    print("="*70 + "\n")


if __name__ == "__main__":
    main()


